{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian Mixture Model with Expectation-Maximization\n",
    "\n",
    "This notebook demonstrates the implementation of a Gaussian Mixture Model (GMM) using the Expectation-Maximization (EM) algorithm from scratch.\n",
    "\n",
    "## Overview\n",
    "\n",
    "A Gaussian Mixture Model assumes that the data comes from a mixture of K multivariate Gaussian distributions. The EM algorithm iteratively estimates:\n",
    "\n",
    "- **E-step**: Compute posterior probabilities (responsibilities) of each data point belonging to each component\n",
    "- **M-step**: Update model parameters (means, covariances, mixing weights) based on responsibilities\n",
    "\n",
    "Let's start by importing the necessary libraries and our custom implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.join('..', 'algorithm'))\n",
    "sys.path.append(os.path.join('..', 'data'))\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Ellipse\n",
    "from matplotlib.colors import ListedColormap\n",
    "import matplotlib.animation as animation\n",
    "\n",
    "from gmm_em import GaussianMixtureEM\n",
    "from synthetic import generate_2d_blobs, generate_elongated_clusters, generate_concentric_circles\n",
    "\n",
    "# Set style for better plots\n",
    "plt.style.use('seaborn-v0_8' if 'seaborn-v0_8' in plt.style.available else 'default')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Generate Synthetic Data\n",
    "\n",
    "Let's create a 2D synthetic dataset with three Gaussian clusters that have different shapes and orientations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate elongated clusters for a challenging dataset\n",
    "X, y_true = generate_elongated_clusters(n_samples=300, random_state=42)\n",
    "\n",
    "print(f\"Generated {len(X)} samples with {len(np.unique(y_true))} true clusters\")\n",
    "print(f\"Data shape: {X.shape}\")\n",
    "print(f\"True cluster sizes: {np.bincount(y_true)}\")\n",
    "\n",
    "# Plot the true clusters\n",
    "plt.figure(figsize=(10, 6))\n",
    "colors = ['red', 'blue', 'green', 'purple', 'orange']\n",
    "for k in range(len(np.unique(y_true))):\n",
    "    mask = y_true == k\n",
    "    plt.scatter(X[mask, 0], X[mask, 1], c=colors[k], alpha=0.7, \n",
    "               label=f'True Cluster {k}', s=50)\n",
    "\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('True Cluster Assignments')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Fit Gaussian Mixture Model\n",
    "\n",
    "Now let's fit our GMM implementation to the data. We'll use 3 components since we know the true number of clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and fit the GMM\n",
    "n_components = 3\n",
    "gmm = GaussianMixtureEM(\n",
    "    n_components=n_components,\n",
    "    max_iter=100,\n",
    "    tol=1e-6,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"Fitting Gaussian Mixture Model...\")\n",
    "gmm.fit(X)\n",
    "\n",
    "print(f\"Converged: {gmm.converged_}\")\n",
    "print(f\"Number of iterations: {gmm.n_iter_}\")\n",
    "print(f\"Final log-likelihood: {gmm.log_likelihood_history_[-1]:.4f}\")\n",
    "print(f\"Mixing weights: {gmm.weights_.round(3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Visualize Results\n",
    "\n",
    "Let's visualize the clustering results and compare them with the true labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions\n",
    "y_pred = gmm.predict(X)\n",
    "probabilities = gmm.predict_proba(X)\n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Plot predicted clusters\n",
    "for k in range(n_components):\n",
    "    mask = y_pred == k\n",
    "    axes[0].scatter(X[mask, 0], X[mask, 1], c=colors[k], alpha=0.7, \n",
    "                   label=f'Predicted Cluster {k}', s=50)\n",
    "\n",
    "axes[0].set_xlabel('Feature 1')\n",
    "axes[0].set_ylabel('Feature 2')\n",
    "axes[0].set_title('GMM Predicted Clusters')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot soft assignments (probabilities)\n",
    "# Use the maximum probability to determine color intensity\n",
    "max_probs = np.max(probabilities, axis=1)\n",
    "scatter = axes[1].scatter(X[:, 0], X[:, 1], c=y_pred, cmap='viridis', \n",
    "                         alpha=max_probs, s=50)\n",
    "\n",
    "axes[1].set_xlabel('Feature 1')\n",
    "axes[1].set_ylabel('Feature 2')\n",
    "axes[1].set_title('Soft Assignments (Opacity = Confidence)')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "plt.colorbar(scatter, ax=axes[1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print cluster statistics\n",
    "print(f\"Predicted cluster sizes: {np.bincount(y_pred)}\")\n",
    "print(f\"Average prediction confidence: {np.mean(np.max(probabilities, axis=1)):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualize Gaussian Components\n",
    "\n",
    "Let's plot the fitted Gaussian components as confidence ellipses to understand how well the model captures the data distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_gaussian_ellipse(mean, cov, ax, color, alpha=0.3, n_std=1):\n",
    "    \"\"\"\n",
    "    Plot confidence ellipse for a 2D Gaussian distribution.\n",
    "    \"\"\"\n",
    "    # Compute eigenvalues and eigenvectors\n",
    "    eigenvals, eigenvecs = np.linalg.eigh(cov)\n",
    "    \n",
    "    # Get the angle of the ellipse\n",
    "    angle = np.degrees(np.arctan2(eigenvecs[1, 0], eigenvecs[0, 0]))\n",
    "    \n",
    "    # Width and height are 2 * sqrt(eigenvalue) * n_std\n",
    "    width, height = 2 * n_std * np.sqrt(eigenvals)\n",
    "    \n",
    "    # Create and add ellipse\n",
    "    ellipse = Ellipse(mean, width, height, angle=angle, \n",
    "                     facecolor=color, alpha=alpha, edgecolor=color, linewidth=2)\n",
    "    ax.add_patch(ellipse)\n",
    "    \n",
    "    # Plot center\n",
    "    ax.plot(mean[0], mean[1], 'o', color=color, markersize=8, markeredgecolor='black')\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Plot data points colored by prediction\n",
    "for k in range(n_components):\n",
    "    mask = y_pred == k\n",
    "    plt.scatter(X[mask, 0], X[mask, 1], c=colors[k], alpha=0.6, \n",
    "               label=f'Cluster {k}', s=30)\n",
    "\n",
    "# Plot Gaussian ellipses at 1 and 2 standard deviations\n",
    "for k in range(n_components):\n",
    "    # 1-sigma ellipse (darker)\n",
    "    plot_gaussian_ellipse(gmm.means_[k], gmm.covariances_[k], plt.gca(), \n",
    "                         colors[k], alpha=0.4, n_std=1)\n",
    "    # 2-sigma ellipse (lighter)\n",
    "    plot_gaussian_ellipse(gmm.means_[k], gmm.covariances_[k], plt.gca(), \n",
    "                         colors[k], alpha=0.2, n_std=2)\n",
    "\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('GMM Components with Confidence Ellipses\\n(Dark: 1σ, Light: 2σ)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.axis('equal')\n",
    "plt.show()\n",
    "\n",
    "# Print component parameters\n",
    "print(\"\\nComponent Parameters:\")\n",
    "for k in range(n_components):\n",
    "    print(f\"\\nComponent {k}:\")\n",
    "    print(f\"  Weight: {gmm.weights_[k]:.3f}\")\n",
    "    print(f\"  Mean: [{gmm.means_[k][0]:.3f}, {gmm.means_[k][1]:.3f}]\")\n",
    "    print(f\"  Covariance:\")\n",
    "    print(f\"    [{gmm.covariances_[k][0,0]:.3f}, {gmm.covariances_[k][0,1]:.3f}]\")\n",
    "    print(f\"    [{gmm.covariances_[k][1,0]:.3f}, {gmm.covariances_[k][1,1]:.3f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Convergence Analysis\n",
    "\n",
    "Let's examine how the log-likelihood evolved during the EM iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot log-likelihood vs iteration\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, len(gmm.log_likelihood_history_) + 1), \n",
    "         gmm.log_likelihood_history_, 'b-o', linewidth=2, markersize=6)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Log-Likelihood')\n",
    "plt.title('EM Algorithm Convergence')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Add convergence info\n",
    "if len(gmm.log_likelihood_history_) > 1:\n",
    "    final_ll = gmm.log_likelihood_history_[-1]\n",
    "    initial_ll = gmm.log_likelihood_history_[0]\n",
    "    improvement = final_ll - initial_ll\n",
    "    plt.text(0.02, 0.98, f'Final LL: {final_ll:.2f}\\nImprovement: {improvement:.2f}', \n",
    "             transform=plt.gca().transAxes, verticalalignment='top',\n",
    "             bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Show convergence statistics\n",
    "if len(gmm.log_likelihood_history_) > 1:\n",
    "    print(f\"Initial log-likelihood: {gmm.log_likelihood_history_[0]:.4f}\")\n",
    "    print(f\"Final log-likelihood: {gmm.log_likelihood_history_[-1]:.4f}\")\n",
    "    print(f\"Total improvement: {gmm.log_likelihood_history_[-1] - gmm.log_likelihood_history_[0]:.4f}\")\n",
    "    \n",
    "    if len(gmm.log_likelihood_history_) > 1:\n",
    "        improvements = np.diff(gmm.log_likelihood_history_)\n",
    "        print(f\"Average per-iteration improvement: {np.mean(improvements):.6f}\")\n",
    "        print(f\"Final iteration improvement: {improvements[-1]:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. EM Algorithm Animation\n",
    "\n",
    "Let's create a step-by-step visualization showing how the GMM components evolve during the EM iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new GMM to track intermediate steps\n",
    "class GMM_Tracker(GaussianMixtureEM):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.iteration_history = []\n",
    "    \n",
    "    def fit(self, X):\n",
    "        X = np.asarray(X)\n",
    "        if X.ndim != 2:\n",
    "            raise ValueError(\"X must be 2-dimensional\")\n",
    "        if X.shape[0] < self.n_components:\n",
    "            raise ValueError(\"Number of samples must be >= n_components\")\n",
    "            \n",
    "        # Initialize parameters\n",
    "        self._initialize_parameters(X)\n",
    "        \n",
    "        # Store initial state\n",
    "        self.iteration_history.append({\n",
    "            'means': self.means_.copy(),\n",
    "            'covariances': self.covariances_.copy(),\n",
    "            'weights': self.weights_.copy(),\n",
    "            'log_likelihood': self._compute_log_likelihood(X)\n",
    "        })\n",
    "        \n",
    "        # EM iterations\n",
    "        self.log_likelihood_history_ = []\n",
    "        prev_log_likelihood = -np.inf\n",
    "        \n",
    "        for iteration in range(self.max_iter):\n",
    "            # E-step\n",
    "            responsibilities = self._e_step(X)\n",
    "            \n",
    "            # M-step\n",
    "            self._m_step(X, responsibilities)\n",
    "            \n",
    "            # Store state\n",
    "            current_log_likelihood = self._compute_log_likelihood(X)\n",
    "            self.log_likelihood_history_.append(current_log_likelihood)\n",
    "            \n",
    "            self.iteration_history.append({\n",
    "                'means': self.means_.copy(),\n",
    "                'covariances': self.covariances_.copy(),\n",
    "                'weights': self.weights_.copy(),\n",
    "                'log_likelihood': current_log_likelihood\n",
    "            })\n",
    "            \n",
    "            # Check convergence\n",
    "            if abs(current_log_likelihood - prev_log_likelihood) < self.tol:\n",
    "                self.converged_ = True\n",
    "                break\n",
    "                \n",
    "            prev_log_likelihood = current_log_likelihood\n",
    "        \n",
    "        self.n_iter_ = iteration + 1\n",
    "        return self\n",
    "\n",
    "# Fit tracker GMM\n",
    "gmm_tracker = GMM_Tracker(n_components=3, max_iter=20, random_state=42)\n",
    "gmm_tracker.fit(X)\n",
    "\n",
    "print(f\"Tracked {len(gmm_tracker.iteration_history)} iterations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create snapshots of EM iterations\n",
    "n_snapshots = min(6, len(gmm_tracker.iteration_history))\n",
    "snapshot_indices = np.linspace(0, len(gmm_tracker.iteration_history)-1, n_snapshots, dtype=int)\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, idx in enumerate(snapshot_indices):\n",
    "    ax = axes[i]\n",
    "    state = gmm_tracker.iteration_history[idx]\n",
    "    \n",
    "    # Plot data points\n",
    "    ax.scatter(X[:, 0], X[:, 1], c='lightgray', alpha=0.6, s=20)\n",
    "    \n",
    "    # Plot Gaussian components\n",
    "    for k in range(n_components):\n",
    "        # Plot ellipse\n",
    "        plot_gaussian_ellipse(state['means'][k], state['covariances'][k], \n",
    "                             ax, colors[k], alpha=0.3, n_std=1)\n",
    "    \n",
    "    iteration_num = idx\n",
    "    ll = state['log_likelihood']\n",
    "    ax.set_title(f'Iteration {iteration_num}\\nLog-likelihood: {ll:.2f}')\n",
    "    ax.set_xlabel('Feature 1')\n",
    "    ax.set_ylabel('Feature 2')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_aspect('equal')\n",
    "\n",
    "plt.suptitle('EM Algorithm Evolution: Gaussian Components Over Iterations', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Comparison with Different Numbers of Components\n",
    "\n",
    "Let's compare how the model performs with different numbers of components using information criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_aic_bic(gmm, X):\n",
    "    \"\"\"Compute AIC and BIC for model selection.\"\"\"\n",
    "    n_samples, n_features = X.shape\n",
    "    n_params = gmm.n_components * (n_features + n_features * (n_features + 1) // 2) + gmm.n_components - 1\n",
    "    \n",
    "    log_likelihood = gmm.log_likelihood_history_[-1]\n",
    "    aic = 2 * n_params - 2 * log_likelihood\n",
    "    bic = n_params * np.log(n_samples) - 2 * log_likelihood\n",
    "    \n",
    "    return aic, bic\n",
    "\n",
    "# Test different numbers of components\n",
    "n_components_range = range(1, 7)\n",
    "results = []\n",
    "\n",
    "for n_comp in n_components_range:\n",
    "    print(f\"Fitting GMM with {n_comp} components...\")\n",
    "    \n",
    "    gmm_test = GaussianMixtureEM(\n",
    "        n_components=n_comp,\n",
    "        max_iter=100,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    gmm_test.fit(X)\n",
    "    aic, bic = compute_aic_bic(gmm_test, X)\n",
    "    \n",
    "    results.append({\n",
    "        'n_components': n_comp,\n",
    "        'log_likelihood': gmm_test.log_likelihood_history_[-1],\n",
    "        'aic': aic,\n",
    "        'bic': bic,\n",
    "        'converged': gmm_test.converged_\n",
    "    })\n",
    "\n",
    "# Plot results\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "n_comps = [r['n_components'] for r in results]\n",
    "log_likelihoods = [r['log_likelihood'] for r in results]\n",
    "aics = [r['aic'] for r in results]\n",
    "bics = [r['bic'] for r in results]\n",
    "\n",
    "# Log-likelihood\n",
    "axes[0].plot(n_comps, log_likelihoods, 'bo-', linewidth=2, markersize=8)\n",
    "axes[0].set_xlabel('Number of Components')\n",
    "axes[0].set_ylabel('Log-Likelihood')\n",
    "axes[0].set_title('Log-Likelihood vs Components')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# AIC\n",
    "axes[1].plot(n_comps, aics, 'ro-', linewidth=2, markersize=8)\n",
    "best_aic_idx = np.argmin(aics)\n",
    "axes[1].axvline(n_comps[best_aic_idx], color='red', linestyle='--', alpha=0.7)\n",
    "axes[1].set_xlabel('Number of Components')\n",
    "axes[1].set_ylabel('AIC')\n",
    "axes[1].set_title(f'AIC vs Components (Best: {n_comps[best_aic_idx]})')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# BIC\n",
    "axes[2].plot(n_comps, bics, 'go-', linewidth=2, markersize=8)\n",
    "best_bic_idx = np.argmin(bics)\n",
    "axes[2].axvline(n_comps[best_bic_idx], color='green', linestyle='--', alpha=0.7)\n",
    "axes[2].set_xlabel('Number of Components')\n",
    "axes[2].set_ylabel('BIC')\n",
    "axes[2].set_title(f'BIC vs Components (Best: {n_comps[best_bic_idx]})')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print results table\n",
    "print(\"\\nModel Comparison Results:\")\n",
    "print(\"Components | Log-Likelihood |     AIC     |     BIC     | Converged\")\n",
    "print(\"-\" * 65)\n",
    "for r in results:\n",
    "    print(f\"    {r['n_components']}      |    {r['log_likelihood']:7.2f}    | {r['aic']:7.2f}   | {r['bic']:7.2f}   |    {r['converged']}\")\n",
    "\n",
    "print(f\"\\nBest number of components:\")\n",
    "print(f\"  AIC: {n_comps[best_aic_idx]} components\")\n",
    "print(f\"  BIC: {n_comps[best_bic_idx]} components\")\n",
    "print(f\"  True: 3 components\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Testing on Different Dataset Types\n",
    "\n",
    "Let's test our GMM implementation on different types of synthetic datasets to understand its strengths and limitations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on different dataset types\n",
    "datasets = [\n",
    "    ('Simple Blobs', generate_2d_blobs(n_samples=300, n_components=3, random_state=42)),\n",
    "    ('Elongated Clusters', generate_elongated_clusters(n_samples=300, random_state=42)),\n",
    "    ('Concentric Circles', generate_concentric_circles(n_samples=300, random_state=42))\n",
    "]\n",
    "\n",
    "fig, axes = plt.subplots(3, 3, figsize=(18, 15))\n",
    "\n",
    "for i, (name, (X_test, y_true_test)) in enumerate(datasets):\n",
    "    # Determine number of components\n",
    "    n_true_components = len(np.unique(y_true_test))\n",
    "    \n",
    "    # Fit GMM\n",
    "    gmm_test = GaussianMixtureEM(\n",
    "        n_components=n_true_components,\n",
    "        max_iter=100,\n",
    "        random_state=42\n",
    "    )\n",
    "    gmm_test.fit(X_test)\n",
    "    y_pred_test = gmm_test.predict(X_test)\n",
    "    \n",
    "    # Plot true clusters\n",
    "    for k in range(n_true_components):\n",
    "        mask = y_true_test == k\n",
    "        axes[i, 0].scatter(X_test[mask, 0], X_test[mask, 1], \n",
    "                          c=colors[k], alpha=0.7, s=30)\n",
    "    axes[i, 0].set_title(f'{name}\\nTrue Clusters')\n",
    "    axes[i, 0].grid(True, alpha=0.3)\n",
    "    axes[i, 0].set_aspect('equal')\n",
    "    \n",
    "    # Plot predicted clusters\n",
    "    for k in range(n_true_components):\n",
    "        mask = y_pred_test == k\n",
    "        axes[i, 1].scatter(X_test[mask, 0], X_test[mask, 1], \n",
    "                          c=colors[k], alpha=0.7, s=30)\n",
    "    axes[i, 1].set_title(f'GMM Predictions\\n(LL: {gmm_test.log_likelihood_history_[-1]:.1f})')\n",
    "    axes[i, 1].grid(True, alpha=0.3)\n",
    "    axes[i, 1].set_aspect('equal')\n",
    "    \n",
    "    # Plot with Gaussian ellipses\n",
    "    axes[i, 2].scatter(X_test[:, 0], X_test[:, 1], c='lightgray', alpha=0.6, s=20)\n",
    "    for k in range(n_true_components):\n",
    "        plot_gaussian_ellipse(gmm_test.means_[k], gmm_test.covariances_[k], \n",
    "                             axes[i, 2], colors[k], alpha=0.3, n_std=1)\n",
    "    axes[i, 2].set_title(f'Gaussian Components\\n({gmm_test.n_iter_} iterations)')\n",
    "    axes[i, 2].grid(True, alpha=0.3)\n",
    "    axes[i, 2].set_aspect('equal')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary and Conclusions\n",
    "\n",
    "This notebook demonstrated our from-scratch implementation of the Gaussian Mixture Model using the Expectation-Maximization algorithm. Here are the key takeaways:\n",
    "\n",
    "### Algorithm Performance\n",
    "- The EM algorithm successfully converged on various datasets\n",
    "- Log-likelihood consistently improved with iterations\n",
    "- The model handled different cluster shapes and orientations well\n",
    "\n",
    "### Model Selection\n",
    "- AIC and BIC can help determine the optimal number of components\n",
    "- Both criteria often (but not always) identify the correct number of clusters\n",
    "- BIC tends to be more conservative, preferring simpler models\n",
    "\n",
    "### Strengths and Limitations\n",
    "- **Strengths**: Works well with elliptical clusters, provides probabilistic assignments, handles overlapping clusters\n",
    "- **Limitations**: Assumes Gaussian distributions, struggles with non-convex shapes (like concentric circles)\n",
    "\n",
    "
